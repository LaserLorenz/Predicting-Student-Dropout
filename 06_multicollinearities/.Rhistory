count_sem = max(sem),
count_be = sum(pstatus == 1),
count_nb = sum(pstatus == 2)
) %>%
filter(
row_number() == 1
) %>%
select(
id,
success,
sex_m,
staat_d,
age,
avg_note,
count_sem,
count_be,
count_nb
) %>%
ungroup() %>%
mutate(
avg_be = ifelse(count_be != 0, count_be/count_sem, 0),
avg_nb = ifelse(count_nb != 0, count_nb/count_sem, 0)
)
# sort core dataset
core <- core[, c(1:6, 10:11, 7:9)]
print("Merge core data with linearized subsets")
# merge student with exam data
merge <- core %>%
left_join(
pnote,
by = c('id' = 'id_pnote')
) %>% left_join(
sem,
by = c('id' = 'id_sem')
) %>% left_join(
pversuch,
by = c('id' = 'id_pversuch')
)
print("Arrange columns in correct order")
# arrange columns by examnumber
# save core column names
colnames_core <- names(merge)[1:11]
# save exam column names
colnames_exams <- names(merge)[12:length(names(merge))]
# sort exam column names
colnames_exams <- sort(colnames_exams)
# concat column names vectors
colnames <- c(colnames_core, colnames_exams)
# select columns following new order
output <- merge[,colnames]
print("Return linearized dataset")
return(output)
}
dataset <- backup %>%
filter(
sem <= 3,
success != 2
) %>%
mutate(
pstatus = ifelse(pstatus == 3, 2, pstatus)
)
dataset <- semi_join(dataset, exam_dataset, by = "pnr") %>%
arrange(id)
dataset <- dataset %>%
group_by(
id, pnr
) %>%
arrange(
sem
) %>%
filter(
row_number() == n()
)
backup <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/dataset.csv",
sep = ";"
)
exam_dataset <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/exams.csv",
sep = ";"
)
dataset <- backup %>%
filter(
sem <= 3,
success != 2
) %>%
mutate(
pstatus = ifelse(pstatus == 3, 2, pstatus)
)
length(backup)-length(dataset)
nrow(backup)-nrow(dataset)
nrow(dataset)
dataset <- semi_join(dataset, exam_dataset, by = "pnr") %>%
arrange(id)
dataset <- semi_join(dataset, exam_dataset, by = "pnr") %>%
arrange(id)
backup <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/dataset.csv",
sep = ";"
)
exam_dataset <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/exams.csv",
sep = ";"
)
dataset <- backup %>%
filter(
sem <= 3,
success != 2
) %>%
mutate(
pstatus = ifelse(pstatus == 3, 2, pstatus)
)
print("Remove exams with too little observaitons")
dataset <- semi_join(dataset, exam_dataset, by = "pnr") %>%
arrange(id)
62653-nrow(dataset)
nrow(dataset)
dataset <- dataset %>%
group_by(
id, pnr
) %>%
arrange(
sem
) %>%
filter(
row_number() == n()
)
61389- nrow(dataset)
dataset_lin <- linearize(dataset, 0)
View(dataset)
backup <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/dataset.csv",
sep = ";"
)
backup <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/dataset.csv",
sep = ";"
)
exam_dataset <- read.csv(
file = "~/Desktop/Masterarbeit_local/02_preanalysis/02_output/exams.csv",
sep = ";"
)
View(backup)
View(exam_dataset)
linearize <- function(dataset = dataset, fill_value = NA){
print("Create linearized subsets for each feature")
pnote <- dataset %>%
select(
id, pnr, pnote
) %>%
spread(
key = pnr,
value = pnote,
fill = fill_value
)
# add "_pnote" after the examnumber in colnames
colnames(pnote) <- paste(colnames(pnote), 'pnote', sep = "_")
# spread sem
sem <- dataset %>%
select(
id, pnr, sem
) %>%
spread(
key = pnr,
value = sem,
fill = fill_value
)
# add "_sem" after the examnumber in colnames
colnames(sem) <- paste(colnames(sem), 'sem', sep = "_")
# spread pversuch
pversuch <- dataset %>%
select(
id, pnr, pversuch
) %>%
spread(
key = pnr,
value = pversuch,
fill = fill_value
)
# add "_pversuch" after the examnumber in colnames
colnames(pversuch) <- paste(colnames(pversuch), 'pversuch', sep = "_")
print("Build core dataset")
# build core dataset containing the student data
# extract values: avg_note, avg_be, avg_nb, count_sem, count_be, count_nb
core = dataset %>%
group_by(
id,
success,
sex_m,
staat_d,
age
) %>%
summarise(
avg_note = mean(pnote),
count_sem = max(sem),
count_be = sum(pstatus == 1),
count_nb = sum(pstatus == 2)
) %>%
filter(
row_number() == 1
) %>%
select(
id,
success,
sex_m,
staat_d,
age,
avg_note,
count_sem,
count_be,
count_nb
) %>%
ungroup() %>%
mutate(
avg_be = ifelse(count_be != 0, count_be/count_sem, 0),
avg_nb = ifelse(count_nb != 0, count_nb/count_sem, 0)
)
# sort core dataset
core <- core[, c(1:6, 10:11, 7:9)]
print("Merge core data with linearized subsets")
# merge student with exam data
merge <- core %>%
left_join(
pnote,
by = c('id' = 'id_pnote')
) %>% left_join(
sem,
by = c('id' = 'id_sem')
) %>% left_join(
pversuch,
by = c('id' = 'id_pversuch')
)
print("Arrange columns in correct order")
# arrange columns by examnumber
# save core column names
colnames_core <- names(merge)[1:11]
# save exam column names
colnames_exams <- names(merge)[12:length(names(merge))]
# sort exam column names
colnames_exams <- sort(colnames_exams)
# concat column names vectors
colnames <- c(colnames_core, colnames_exams)
# select columns following new order
output <- merge[,colnames]
print("Return linearized dataset")
return(output)
}
dataset <- backup %>%
filter(
sem <= 3,
success != 2
) %>%
mutate(
pstatus = ifelse(pstatus == 3, 2, pstatus)
)
print("Remove exams with too little observaitons")
dataset <- semi_join(dataset, exam_dataset, by = "pnr") %>%
arrange(id)
print("Remove duplicate exams")
dataset <- dataset %>%
group_by(
id, pnr
) %>%
arrange(
sem
) %>%
filter(
row_number() == n()
)
View(dataset)
dataset_lin <- linearize(dataset, 0)
fill = 0
pnote <- dataset %>%
select(
id, pnr, pnote
) %>%
spread(
key = pnr,
value = pnote,
fill = fill_value
)
pnote <- dataset %>%
select(
id, pnr, pnote
) %>%
spread(
key = pnr,
value = pnote,
fill = fill_value
)
pnote <- dataset %>%
select(
id, pnr, pnote
)
pnote <- dataset %>% select(id, pnr, pnote)
pnote <- dataset %>% select(dataset$id, dataset$pnr, dataset$pnote)
library(dplyr)        # data wrangling
library(tidyr)        # data wrangling
pnote <- dataset %>% select(dataset$id, dataset$pnr, dataset$pnote)
x <- dataset %>% select(id)
source('~/Desktop/Masterarbeit_local/06_multicollinearities/v68.R')
build_ds_traintest <- function(dataset = dataset, n , p = 0.7, balanced = FALSE, replace = FALSE){
if(balanced){
sample1 <- subset(dataset, success == 1)
sample0 <- subset(dataset, success == 0)
sample0 <- sample0[sample(1:nrow(sample0), size = n/2, replace = replace),]
sample1 <- sample1[sample(1:nrow(sample1), size = n/2, replace = replace),]
my_sample <- rbind(sample0, sample1)
rm(sample0, sample1)
}
else {
my_sample <- dataset[sample(1:nrow(dataset), size = n, replace = replace),]
}
# train and test datasets:
samp <- sample(nrow(my_sample), p * nrow(my_sample))
train <- my_sample[samp, ]
test <- my_sample[-samp, ]
traintest <- list(train, test)
return(traintest)
}
library(caret)
library(dplyr)
library(lmtest)
dataset <- dataset %>%
mutate(
success = as.factor(success),
staat_d = as.factor(staat_d),
sex_m = as.factor(sex_m)
)
View(dataset)
set.seed(2)
ttlist <- build_ds_traintest(
dataset_vif,
n = nrow(dataset_vif),
p = 0.7,
balanced = FALSE,
replace = FALSE
)
ttlist <- build_ds_traintest(
dataset,
n = nrow(dataset),
p = 0.7,
balanced = FALSE,
replace = FALSE
)
train <- as.data.frame(ttlist[1])
test <- as.data.frame(ttlist[2])
rm(ttlist)
res <- predict(
model_nul,
newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
pred = predict(
model, newdata = test
)
pred = predict(
model_nul,
newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
red = predict(
model,
newdata = test
)
pred = predict(
model_nul,
newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
confusionMatrix(
data=pred,
test$success
)
pred = predict(
model_nul,
newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
confusionMatrix(
data = pred,
test$success
)
rm(res)
accuracy
167+777
pred
?predict
ttlist <- build_ds_traintest(
dataset,
n = 5000,
p = 0.7,
balanced = TRUE,
replace = TRUE
)
train <- as.data.frame(ttlist[1])
test <- as.data.frame(ttlist[2])
rm(ttlist)
f <- as.formula(paste('success ~', paste(colnames(train)[c(5:ncol(train))], collapse = '+')))
run_logreg(train, test, f)
run_logreg <- function(test = test, train = train, f = f){
# train general logistic model
model <- train(
data = train,
method = "glm",
family = "binomial",
f
)
# view coefficients of model
exp(coef(model$finalModel))
print("Summary of model:")
print(summary(model$finalModel))
print("RSS:")
print(deviance(model$finalModel))
print("Anova:")
print(anova(model$finalModel))
# test model: binary (for probabilistic model add: type = "prob")
res <- predict(
model,
newdata = test
)
# Test model on test data
# Cassification rate
pred = predict(
model, newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
# Confusion matrix
red = predict(model, newdata = test)
confusionMatrix(data=pred, test$success)
}
run_logreg(train, test, f)
run_logreg <- function(test = test, train = train, f = f){
# train general logistic model
model <- train(
data = train,
method = "glm",
family = "binomial",
f
)
print("RSS:")
#print(deviance(model$finalModel))
print("Anova:")
print(anova(model$finalModel))
# test model: binary (for probabilistic model add: type = "prob")
res <- predict(
model,
newdata = test
)
# Test model on test data
# Cassification rate
pred = predict(
model, newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
# Confusion matrix
red = predict(model, newdata = test)
confusionMatrix(data=pred, test$success)
}
run_logreg(train, test, f)
run_logreg <- function(test = test, train = train, f = f){
# train general logistic model
model <- train(
data = train,
method = "glm",
family = "binomial",
f
)
print("RSS:")
#print(deviance(model$finalModel))
print("Anova:")
#print(anova(model$finalModel))
# test model: binary (for probabilistic model add: type = "prob")
res <- predict(
model,
newdata = test
)
# Test model on test data
# Cassification rate
pred = predict(
model, newdata = test
)
accuracy <- table(
pred,
test[,"success"]
)
# Confusion matrix
red = predict(model, newdata = test)
confusionMatrix(data=pred, test$success)
}
run_logreg(train, test, f)
View(dataset %>% group_by(success) %>% summarise(count = n()))
590/3146
1-(590/3146)
